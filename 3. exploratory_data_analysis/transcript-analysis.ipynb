{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRANSCRIPT ANALYSIS\n",
    "\n",
    "The following code is for performing analysis on the transcripts of the corpus. This notebook was written for the **INTERSPEECH 2025** datasets. It is compatible for both the Regional Speech corpus and the Ben10 corpus made for the interspeech 2025 conference and *any* dataset that follows a similar file structure\n",
    "\n",
    "\n",
    "```\n",
    "|- dev\n",
    "|    |- all.xlsx/dev.xlsx\n",
    "|    |- train.xlsx/dev_train.xlsx\n",
    "|    |- ...\n",
    "|- train\n",
    "|    |- train_barishal.wav\n",
    "|    |- ...\n",
    "|- test\n",
    "|- valid\n",
    "|- ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T20:54:31.912868Z",
     "iopub.status.busy": "2025-02-19T20:54:31.912466Z",
     "iopub.status.idle": "2025-02-19T20:54:51.743871Z",
     "shell.execute_reply": "2025-02-19T20:54:51.742614Z",
     "shell.execute_reply.started": "2025-02-19T20:54:31.912836Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.5/26.5 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kaggle-environments 1.16.11 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\n",
      "mlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n",
      "plotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\n",
      "scikit-image 0.25.0 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install bnlp-toolkit -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T20:54:51.745837Z",
     "iopub.status.busy": "2025-02-19T20:54:51.745514Z",
     "iopub.status.idle": "2025-02-19T20:55:18.380959Z",
     "shell.execute_reply": "2025-02-19T20:55:18.379762Z",
     "shell.execute_reply.started": "2025-02-19T20:54:51.745810Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "from typing import Optional\n",
    "import string\n",
    "from IPython.display import display\n",
    "\n",
    "from bnlp import NLTKTokenizer\n",
    "tokenizer = NLTKTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-19T21:03:01.730826Z",
     "iopub.status.busy": "2025-02-19T21:03:01.730397Z",
     "iopub.status.idle": "2025-02-19T21:03:01.741316Z",
     "shell.execute_reply": "2025-02-19T21:03:01.740167Z",
     "shell.execute_reply.started": "2025-02-19T21:03:01.730796Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# DURATION FUNCTIONS\n",
    "##############################\n",
    "def hms_format(s):\n",
    "    h, s = divmod(s, 3600)\n",
    "    m, s = divmod(s, 60)\n",
    "    h, m, s = int(h), int(m), round(s)\n",
    "    return f\"{h}:{m:02}:{s:02}\"\n",
    "\n",
    "def duration(dataframe, disable_tqdm:bool = False) -> dict[str, int|float]:\n",
    "    total_seconds: float = 0.0\n",
    "    for path in tqdm(dataframe.path, disable=disable_tqdm):\n",
    "        total_seconds += librosa.get_duration(path=path)\n",
    "    result = {}\n",
    "\n",
    "    return {\n",
    "        \"total_sec\" : round(total_seconds, 2),\n",
    "        \"total_hours\" : round(total_seconds / 3600, 2),\n",
    "        \"total_min\" : round(total_seconds / 60, 2),\n",
    "        \"avg_duration\" : round(total_seconds / dataframe.shape[0], 2),\n",
    "        \"total_duration\" : hms_format(total_seconds)\n",
    "    }\n",
    "\n",
    "\n",
    "##############################\n",
    "# OOD FUNCTIONS\n",
    "##############################\n",
    "\n",
    "def tokenize_word(sentence:str) -> list:\n",
    "    return tokenizer.word_tokenize(\n",
    "        sentence.translate(str.maketrans('!#$%&()*+,./:;<=>?@[\\\\]^_`{|}~।', \"                              \")) # replace punc with space\n",
    "    )\n",
    "\n",
    "\n",
    "def map_word_to_frequency(dataframe, disable_tqdm:bool = False) -> dict[str, float]:\n",
    "    result = {}\n",
    "    for sen in tqdm(dataframe.transcripts, disable=disable_tqdm):\n",
    "        words = tokenize_word(sen)\n",
    "        for word in words:\n",
    "            try:\n",
    "                result[word] += 1\n",
    "            except KeyError:\n",
    "                result[word] = 1\n",
    "    return result\n",
    "\n",
    "\n",
    "def find_ood(\n",
    "    word_dict:dict, standard:set,\n",
    ") -> tuple[dict, dict]:\n",
    "    temp = set(word_dict.keys()).difference(standard)\n",
    "    ood_words = {}\n",
    "    for word in temp:\n",
    "        ood_words[word] = word_dict[word]\n",
    "\n",
    "    return (\n",
    "        ood_words,\n",
    "        {\n",
    "            \"ood_unq_word_count\" : len(ood_words.keys()),\n",
    "            \"ood_total_word_count\" : sum(ood_words.values()),\n",
    "            \"ood_unq_word_percent\" : len(ood_words.keys()) / len(word_dict.keys()) * 100,\n",
    "            \"ood_total_word_percent\" : sum(ood_words.values()) / sum(word_dict.values()) * 100,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load standard bengali words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "standard_bangla = pd.read_csv('/kaggle/input/bengaliai-train-csv/train.csv')\n",
    "words = []\n",
    "for sentence in tqdm(standard_bangla.sentence):\n",
    "    words += tokenize_word(sentence)\n",
    "\n",
    "STANDARD_BANGLA_WORDS = set(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# District wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-18T15:08:26.252825Z",
     "iopub.status.busy": "2025-02-18T15:08:26.252457Z",
     "iopub.status.idle": "2025-02-18T15:08:29.955368Z",
     "shell.execute_reply": "2025-02-18T15:08:29.954001Z",
     "shell.execute_reply.started": "2025-02-18T15:08:26.252794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "districts = [\"Rangpur\", \"Kishoreganj\", \"Narail\", \"Chittagong\", \"Narsingdi\", \"Tangail\", \"Habiganj\",\"Barishal\", \"Sandwip\", \"Sylhet\", \"Comilla\", \"Noakhali\"]\n",
    "df = pd.read_excel(\"/kaggle/input/interspeech-2025/dev/dev.xlsx\")\n",
    "path = \"/kaggle/input/interspeech-2025\"\n",
    "splits = [\"train\", \"test\", \"valid\", \"all\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df[\"path\"] = df[[\"file_name\", \"split_type\"]].apply(lambda x: os.path.join(path, x.split_type.lower(), x.file_name), axis=1)\n",
    "\n",
    "for district in districts:\n",
    "    print(f\"====================================================== {district} ======================================================\")\n",
    "    result = pd.DataFrame()\n",
    "    for split in splits:\n",
    "        print(split, \": \", end=\"\")\n",
    "        if split == \"all\":\n",
    "            df2 = df.query(\"district == @district\").copy()\n",
    "        else:\n",
    "            df2 = df.query(\"district == @district and split_type == @split\").copy()\n",
    "        \n",
    "        duration_result = duration(df2, disable_tqdm=True)\n",
    "        regional_words = map_word_to_frequency(df2, disable_tqdm=True)\n",
    "        ood_words, ood_stats = find_ood(regional_words, STANDARD_BANGLA_WORDS)\n",
    "        \n",
    "        data = {**duration_result, **ood_stats}\n",
    "        data[\"total_words\"] = sum(regional_words.values())\n",
    "        data[\"wpm\"] = sum(regional_words.values()) / duration_result[\"total_min\"]\n",
    "        data[\"wps\"] = sum(regional_words.values()) / df2.shape[0]\n",
    "        data[\"count\"] = df2.shape[0]\n",
    "\n",
    "        result = pd.concat([result, pd.DataFrame(data, index=[split])])\n",
    "        print(\"Done\")\n",
    "    display(result)\n",
    "    print()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOTAL CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"/kaggle/input/interspeech-2025/dev/dev.xlsx\")\n",
    "df[\"path\"] = df[[\"file_name\", \"split_type\"]].apply(lambda x: os.path.join(\"/kaggle/input/interspeech-2025\", x.split_type.lower(), x.file_name), axis=1)\n",
    "\n",
    "\n",
    "result = pd.DataFrame()\n",
    "for split in [\"train\", \"test\", \"valid\", \"all\"]:\n",
    "    print(split)\n",
    "    if split == \"all\":\n",
    "        df2 = df.copy()\n",
    "    else:\n",
    "        df2 = df.query(\"split_type == @split\")\n",
    "    \n",
    "    duration_result = duration(df2)\n",
    "    regional_words = tokenize_word(df2)\n",
    "    ood_words, ood_stats = find_ood(regional_words, STANDARD_BANGLA_WORDS)\n",
    "    \n",
    "    data = {**duration_result, **ood_stats}\n",
    "    data[\"total_words\"] = sum(regional_words.values())\n",
    "    data[\"wpm\"] = sum(regional_words.values()) / duration_result[\"total_min\"]\n",
    "    data[\"wps\"] = sum(regional_words.values()) / df2.shape[0]\n",
    "    data[\"count\"] = df2.shape[0]\n",
    "\n",
    "    result = pd.concat([result, pd.DataFrame(data, index=[split])])\n",
    "\n",
    "display(result)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 5472686,
     "sourceId": 9072709,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6544256,
     "sourceId": 10672472,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
